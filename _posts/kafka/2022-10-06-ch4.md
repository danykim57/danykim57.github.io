Producer and sending messages.

First of all, Apache Kafka does not provide a concrete way to implement producer.

Let's go over with the one of the most used protocols, SMTP

When a user sends an email, usually the email goes through SMTP and stored 

in the one of the email servers, and another user will read the email.

Why would somebody need to use Kafka, rather than the simple implementation of email service application.

Case 1: You need to do live-analysis from the email
Case 2: Expectation of heavy requests that might put the email in danger of lost
Case 3: Re-processing the email for other purposes



Producer's duties
 1. Put metadata about the cluster
 2. Keep the promise of sending times with acks along the configuration
 3. Be aware of idempotent option

Producer's options
 1. acks:  Number of replica acknowledgments that a producer requires before success is established
 2. bootstrap.servers: One or more Kafka brokers to connect for startup 
 3. key.serializer: The class that’s used for serialization of the key
 4. value.serializer: The class that’s used for serialization of the value

The way how Producer finds the broker leader

 1. Producer get a connection to bootstrap servers
 2. One of the brokers sends back metadata to producer, which is the broker leader 

When acks options is 0
 1. Producer writes to the partition leader
 2. The leader doesn't wait whether the write was successful or not
 3. We will never know the state of any replica copies

Sample of Producer Generation 

```JAVA
public class AuditProducer {
...
private static final Logger log = LoggerFactory.getLogger
(AuditProducer.class);Properties kaProperties = new Properties();
kaProperties.put( "bootstrap.servers",
"localhost:8084,localhost:8085,localhost:8086");
kaProperties.put("acks", "all");
kaProperties.put("retries", "3");
kaProperties.put("max.in.flight.requests.per.connection", "1");
```

Callback implementation

```JAVA
public class AlertCallback implements Callback {
  private static final Logger log =
    LoggerFactory.getLogger(AlertCallback.class);
  public void onCompletion
      (RecordMetadata metadata,
       Exception exception) {
    if (exception != null) {
      log.error("kinaction_error", exception);
    } else {
      log.info("kinaction_info offset = {}, topic = {}, timestamp = {}",
          metadata.offset(), metadata.topic(), metadata.timestamp());
    }
  }
```

Read Kafka config from a file
```java
Properties kaProperties = readConfig();
String topic = kaProperties.getProperty("topic");
kaProperties.remove("topic");
try (Producer<String, String> producer =
                        new KafkaProducer<>(kaProperties)) {
  ProducerRecord<String, String> producerRecord =
    new ProducerRecord<>(topic, null, "event");
producer.send(producerRecord,
                new AlertCallback());
}
private static Properties readConfig() {
    Path path = Paths.get("src/main/resources/kafkasink.conf");
    Properties kaProperties = new Properties();
    try (Stream<String>  lines = Files.lines(path))
        lines.forEachOrdered(line ->
            determineProperty(line, kaProperties));
    } catch (IOException e) {
        System.out.println("kinaction_error" + e);
    }
    return kaProperties;
}
private static void determineProperty
    (String line, Properties kaProperties) {
    if (line.contains("bootstrap")) {
        kaProperties.put("bootstrap.servers", line.split("=")[1]); } 
    else if (line.contains("acks")) {
        kaProperties.put("acks", line.split("=")[1]);
    } else if (line.contains("compression.type")) {
        kaProperties.put("compression.type", line.split("=")[1]); } 
    else if (line.contains("topic")) {
    }
        kaProperties.put("topic", line.split("=")[1]);
    }
```