Fictional Scenario

- The current project ignores the most of the messages out of the amount and frequency of message creations

Slow implementation with Kafka Connection

- Just apply Kafka Connection on the one DB table
- Migrating data from DB table to Kafka by Kafka Connection
- Use Pre-built class, FileStreamSource 

File source config
```
name=alert-source
connector.class=FileStreamSource
tasks.max=1
file=alert.txt
topic=kinaction_alert_connect
```

Data moves from DB to topic

Connect starts like this

```
bin/connect-standalone.sh config/connect-standalone.properties \
alert-source.properties
```

Sample SQL file
```
CREATE TABLE invoices(
id INT PRIMARY KEY NOT NULL,
title TEXT NOT NULL,
details CHAR(50),
billedamt REAL,
modified TIMESTAMP DEFAULT (STRFTIME('%s', 'now')) NOT NULL
);

INSERT INTO invoices (id,title,details,billedamt) \
VALUES (1, 'book', 'Franz Kafka', 500.00 );
```

DB table source Connect
```
confluent-hub install confluentinc/kafka-connect-jdbc:10.2.0
confluent local services connect start 
...
# See Commands.md for other steps
confluent local services connect connector config jdbc-source 
--config etc/kafka-connect-jdbc/kafkatest-sqlite.properties
```

Avro: row-oriented remote procedure call and data serialization framework developed within Apache's Hadoop project

Avro is used here mainly as data serialization system.

It is not the only option available for Kafka, but most likely we will see in the real implementation in the industry.

Putting dependency in pom.xml for Java project

Example:

```
<dependency>
<groupId>org.apache.avro</groupId>
<artifactId>avro</artifactId>
<version>${avro.version}</version>
</dependency>
```

Putting Avro Maven plungin in pom.xml

```
<plugin>
<groupId>org.apache.avro</groupId>
<artifactId>avro-maven-plugin</artifactId>
<version>${avro.version}</version>
<executions>
<execution>
<phase>generate-sources</phase>
<goals>
<goal>schema</goal>
</goals>
...
</execution>
</executions>
</plugin>
```

Consumer Java code example:

```java
public class HelloWorldConsumer {
    final static Logger log = LoggerFactory.getLogger(HelloWorldConsumer.class);
    private volatile boolean keepConsuming = true;

    public static void main(String[] args) {
        Properties kaProperties = new Properties();
        kaProperties.put("bootstrap.servers", "localhost:9094");
        kaProperties.put("key.deserializer",
                "org.apache.kafka.common.serialization.LongDeserializer");
        kaProperties.put("value.deserializer",
                "io.confluent.kafka.serializers.KafkaAvroDeserializer");
        kaProperties.put("schema.registry.url", "http://localhost:8081");
        HelloWorldConsumer helloWorldConsumer = new HelloWorldConsumer();
        helloWorldConsumer.consume(kaProperties);
        Runtime.getRuntime()
                .addShutdownHook(
                        new Thread(helloWorldConsumer::shutdown)
                );
    }

    private void consume(Properties kaProperties) {
        try (KafkaConsumer<Long, Alert> consumer =
                     new KafkaConsumer<>(kaProperties)) {
            consumer.subscribe(
                    List.of("kinaction_schematest")
            );
            while (keepConsuming) {
                ConsumerRecords<Long, Alert> records =
                        consumer.poll(Duration.ofMillis(250));
                for (ConsumerRecord<Long, Alert> record :
                        records) {
                    log.info("kinaction_info offset = {}, kinaction_value = {}",
                            record.offset(),
                            record.value());
                    private void shutdown() {
                        keepConsuming = false;
                    }
                }
            }
        }
    }
}
```

