Fictional Scenario

- The current project ignores the most of the messages out of the amount and frequency of message creations

Slow implementation with Kafka Connection

- Just apply Kafka Connection on the one DB table
- Migrating data from DB table to Kafka by Kafka Connection
- Use Pre-built class, FileStreamSource 

File source config
```
name=alert-source
connector.class=FileStreamSource
tasks.max=1
file=alert.txt
topic=kinaction_alert_connect
```

Data moves from DB to topic

Connect starts like this

```
bin/connect-standalone.sh config/connect-standalone.properties \
alert-source.properties
```

Sample SQL file
```
CREATE TABLE invoices(
id INT PRIMARY KEY NOT NULL,
title TEXT NOT NULL,
details CHAR(50),
billedamt REAL,
modified TIMESTAMP DEFAULT (STRFTIME('%s', 'now')) NOT NULL
);

INSERT INTO invoices (id,title,details,billedamt) \
VALUES (1, 'book', 'Franz Kafka', 500.00 );
```

DB table source Connect
```
confluent-hub install confluentinc/kafka-connect-jdbc:10.2.0
confluent local services connect start 
...
# See Commands.md for other steps
confluent local services connect connector config jdbc-source 
--config etc/kafka-connect-jdbc/kafkatest-sqlite.properties
```